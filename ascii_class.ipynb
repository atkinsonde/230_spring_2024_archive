{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIacUF4BPCpt3qf4DBMipH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atkinsonde/230/blob/main/ascii_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code that we used in GEOG/EOS 230 class**<br>\n",
        "**Thursday Jan 18 2024**\n",
        "\n",
        "Recall that we looked at a number of types of ASCII files along with a variety of the pandas read functions.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V6fjcfeiocbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##What is coding?\n",
        "Before we begin...\n",
        "One thing I haven't made clear is why we are coding at all, and what exactly these pandas reading functions are doing. We can compare coding to opening a dataset using a spreadsheet. With a spreadsheet we can see the data in the cells. However, the data still have to be in a format that excel understands. Some are simple: excel understands a csv file and automatically gives a separate cell for each value between commas on each row. That seems pretty straightforward because you can just , but excel is still undergoing a process of *importing* the data so it is compatible with its format. It is less obvious because we can still see all the data in the sheet.\n",
        "\n",
        "\n",
        "We need to convert our dataset into a format that python understands. By \"understanding\" I mean a format that it can operate its functions with. Data that we convert, or read in, is assigned to a variable. Recall that this is a process whereby python opens up a space in memory, reserves it, and gives it the name that we just gave it. This is what \"reading\" means, and what all of these \"read_XXX()\" functions do in pandas - they convert different file types into a form python can understand.\n",
        "\n",
        "\n",
        "From there, we can now do all our stats, plots, and analyses on the data stored in the new variable.\n"
      ],
      "metadata": {
        "id": "9j0fMis1pPHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mount class files to your COLAB drive\n",
        "Here is how to get files into your colab so you can refer to them in your code. To do this we will be using the *GitHub repository* I have set up for the class. GitHub is a major professional code \"repo\" and team (or individual) development environment.\n",
        "\n",
        "\n",
        "1.   Run this code: ` !git clone https://github.com/atkinsonde/230.git` in one of your cells. I've already put it in. Click in the cell and type the Control key (CTRL)+Enter at the same time to run the code in any one cell (OR just click on the black circle with the triangle).\n",
        "\n",
        "2.   Once you have run the git code look on the menu on the left. Look for the key symbol and click the folder underneath to open up a window to the left of your code window (if the window is already open, clicking the folder will collapse it and widen your coding area).\n",
        "\n",
        "3.   With the window open, you will see at the top the word \"Files\" and a series of four icons below it. Below, you will see a folder called \"sample_data\". Click on the refresh folder icon (the one with the little circle arrow on it, not the recycle one). Another folder will appear called \"230\". (The little triangles on the left will rotate down when the folders are open). <br><br>\n",
        "\n",
        "4.   If you click on \"230\" you will see a sub-folder called \"ascii_class\". If you click on that, you will see a series of files. These are the files we worked with in class.\n",
        "\n",
        "5.   Your path will therefore be: \"230/ascii_class/FILENAME.EXT\", where \"FILENAME.EXT\" is replaced with the filename you want, such as \"230/ascii_class/combined.json\". Look at the code I have below.\n",
        "\n",
        "\n",
        "The \"refresh\" folder icon looks like this: <br>\n",
        "\n",
        "![refresh_folder.PNG](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFQAAABPCAYAAAB1euKZAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAAKmSURBVHhe7dnfa3lxHMfx/atyQUm07Gau5EYp5cKNlCs3mwvSptT8KMXFSo0S29Vqyo2QuDDe3z6ffSyWNfV9dY6j17PemTOfc/LYsXF2JQwaQcERFBxBwREUHEHBERQcQcERFBxBwREUHEHBERQcQcERFBxBwREUHEHBERQcQcERFBxBwREUHEHBERQcQcERFBxBwREUHEHBERSc5aClUkmur6/F5XL9OqFQSCaTiVnhrCwFfXl5OQp4bG5ubhyJaimoOjuP4f02CrVQKJw0jUZDFouFOZJ9WQqqnvgxONQEAgGZzWbmaPZ0UaBqnp6ezNHsyTZQdLv9qmPYGUHBERTcxYDGYjE9zWbTbLGniwE9lxwJ2u12JZ/PSzKZ1Le9Xk9vr1Qqet/q1q4cBareY8bj8e997E8ikdC46muCntByudSfnNTaSCQij4+P+sysVqv6/m6/agh6QqlUSq+7u7uT7XZrtn69zNWZ6fV6CXpq8/lcrwmHwweYKnVlarfP3RD0j1qtll6jXuY/e3t7k36/fzB2XqVyBGi73dZrHh4ezJbzzVLQ/ct3CveUUX+MBoOBXpPJZMyejlev1/Wa6XRqtlifpaCdTucb9NQpFouyXq8lGAyK2+2W8Xhs9nbYcDjUj1fvBH7+nrUyS0HVE81ms3/+C2R/fD6frFYrqdVq+r6CHY1GZo9fPT8/i9/v198vl8tmqz1ZCvo/qR9GLpfTaOpMjUaj+rP77e3tN/79/b1sNhuzwp4cA7pLvZHfR/R4PJJOp+Xj48M8wt4cB7rr/f1dXl9f5fPz02w5jxwLeq4RFBxBwREUHEHBERQcQcERFBxBwREUHEHBERTclbowy8ENz1BwBAVHUHAEBUdQcAQFR1BwBAVHUHAEBUdQcASFJvIPJtuJosBjdYQAAAAASUVORK5CYII=)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L1C2ibr51Hc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/atkinsonde/230.git"
      ],
      "metadata": {
        "id": "4535T9Rhj-Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import libraries\n",
        "\n",
        "After \"mounting\" the data drive using the !git, the next thing to do is to import the libraries that contain the functions we will need. Recall that \"base\" python has a basic suite of progam control commands and functions, but not too much else for specific uses. To get that extra functionality we have to specify particular libraries.\n",
        "\n",
        "The command is `import LIBNAME` and usually we include `as SHORT_LIBNAME` to give it a handle that is easier to type out.\n",
        "\n",
        "The whole thing becomes `import pandas as pd` or `import numpy as np` and so on.\n",
        "\n",
        "Run this code:"
      ],
      "metadata": {
        "id": "yyfJlVAvd2uu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ba78k-rAoP0S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that when you run it you can see the black circle icon on the left take on a \"busy\" appearance. When the code is finished, it returns to a black circle with an arrow. Note as well that no output appears for this particular command. You know it has run correctly if it gives you no output.\n",
        "\n"
      ],
      "metadata": {
        "id": "MDvEbpeIfYM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reader: `pd.read_csv()`\n",
        "\n",
        "Now for the first pandas reader, `pd.read_csv()`"
      ],
      "metadata": {
        "id": "MPQVm1Mtoxm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\"230/ascii_class/oliver_dump_2016-2019.csv\")"
      ],
      "metadata": {
        "id": "6VyHtTQVxpkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This reads in some weather station data from a station I had in a merlot field in a vineyard near Oliver, BC. It is a standard, simple-structure csv file. Each row is one observation, and each observation is made for several variables.\n",
        "\n",
        "Run this and note three things:\n",
        "1. A new cell with output from the command appears. This is what you could call \"output to the screen\".  \n",
        "2. All you have is output to the screen - no new variable storing the data has been created.\n",
        "3. I don't always want to see the output; if you want to remove it, move your mouse over the little icon (box with a right-pointing arrow), it will turn into an X - click that and the output box will disappear.\n",
        "\n",
        "Now let's save these data to a variable. Run the cell below:"
      ],
      "metadata": {
        "id": "UKgU9IQumBTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merlot_data = pd.read_csv(\"230/ascii_class/oliver_dump_2016-2019.csv\")"
      ],
      "metadata": {
        "id": "RSc6okHRm5CC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note this time there was no output. But, if you click on the {*X*} icon on the far left edge, which shows you your variables, you will see a new variable has appeared. It's name is merlot_data and its \"shape\" is (14608,7), which means 14,608 rows by 7 columns. Each row is an observation taken on the hour, and each column is a variable.\n",
        "Instead of dumping the output to the screen, because we *assigned* the output to a variable using the = sign, python took the result of the reading process and stored it in the variable."
      ],
      "metadata": {
        "id": "IxwFZ9Mrnw0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reader: pd.read_fwf()\n",
        "\n",
        "The next reader we looked at was the fixed-width field reader. This lets us handle files that my not have a separator but which have a perfectly regular column width structure.\n",
        "\n",
        "This is a more complicated one to set up, so recall I said good coding practice is to build up your solution in parts. Add one thing, run it, make sure it works, then add the next thing, etc. So, the first thing is to make sure the `pd.read_fwf()` works at all for this, and to make sure python can find the file:"
      ],
      "metadata": {
        "id": "XAp4AxXTp3F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_fwf(\"230/ascii_class/event_data_structure.txtx\")"
      ],
      "metadata": {
        "id": "Sv2YcWehq7lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall from our discussion in class that python is reading each row as one big \"string\". It doesn't understand any particular variable. You have to manually specify the column ranges that define each variable, and then you have to provide a name for each variable. Recall we do this with two additional *arguments* to the `pd.read_fwf()` function: `colspecs=` and `names=`. (separate each argument with a comma). I will do the first one and leave it as a self-help exercise for you to do the rest the variables. You will also note that the first row is a header value. Use the online help to find the argument that allows you to skip over rows. Here is that comprehensive website [pandas.pydata.org](https://pandas.pydata.org/docs/user_guide/io.html#)). Search for `read_fwf()`, and note that many of the arguments are the same for a number of the readers.\n",
        "\n",
        "Here is your metadata.<br>\n",
        "There are eight variables in total:\n",
        "- yyyy is a four column year\n",
        "- jjs  is a three column julian day storm event start\n",
        "- jje  is a three column julian day storm event end\n",
        "- dur  is a three column storm event duration in hours\n",
        "- av1  is a three column storm max intensity mean wind speed in m/s\n",
        "- ava  is a three column storm total mean wind speed in m/s\n",
        "- lonnn is a five column longitude (multiplier 0.1)\n",
        "- lat is a three column latitude (multiplier 0.1)\n",
        "\n",
        "(you don't have to apply the multipliers etc).\n",
        "\n",
        "I'll start you off:"
      ],
      "metadata": {
        "id": "OPk9PDGrruSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_fwf(\"230/ascii_class/event_data_structure.txtx\",\n",
        "            colspecs=[(0,4)],\n",
        "            names=['year'])"
      ],
      "metadata": {
        "id": "_zeEdCjrxYNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that all of these pandas readers are creating what is called a *DataFrame* object. `colspecs=` takes a *list* object [] as a variable; that list object in turn contains *tuple* objects with column start and end pairs (and recall python starts counting at 0 and stops one before the end, so 4 will give you up to column #3). Note that you are free to structure your code writing such that each argument is listed under the last. It is much easier to read this way.\n",
        "\n",
        "You go ahead a finish that, and when it is reading all of the variables and getting rid of the header, save it to a variable and verify it is in your list of variables on the left {X}."
      ],
      "metadata": {
        "id": "IcZeDpXux1WT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reader: pd.read_html() and indexing\n",
        "\n",
        "This is the first of the markup-language readers, <b>hyper-text markup language</b>. The example for this one demonstrated that we can give the python file readers a direct link to an html web address. We will pull the \"failed banks\" file from the US FDIC website.\n"
      ],
      "metadata": {
        "id": "kTsQXHYZzZDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_html(\"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\")"
      ],
      "metadata": {
        "id": "zitCBUge0M6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works, so assign it to a variable:"
      ],
      "metadata": {
        "id": "u2WVtzbJ-2nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks = pd.read_html(\"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\")"
      ],
      "metadata": {
        "id": "u9LGG7oV-7j7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and verify your new variable has been created over at {X}.\n",
        "\n",
        "Always be checking for details... what is the value for the Shape parameter? Note also the Type parameter. It is saying it is a list object with a single item in the list. Also, mouse over the name of the variable; it gives you more info and gives a peek at the data in the object. In fact we can see the [ which is used to enclose a List object. This lets us learn a little more about the list object - anything can be a member of a list, including an entire DataFrame. That is what is happening here: it has read in the table into a DataFrame that is then stored inside a List object (don't ask me why...).\n",
        "\n",
        "So - if we want to use the DataFrame we basically have to extract it from the List. This introduces us to the concept of *Indexing*. Indexing is selecting specific items from within an object. In this case, we want to select the one item from the List object (it seems weirdly redundant, but that's just how it works). We use the [] to provide index values, like: `failed_banks[0]` (when the [] are put beside a variable they mean an index, not a list). `failed_banks[0]` will pull out the first item in the `failed_banks` list object. Let's try it:"
      ],
      "metadata": {
        "id": "63jTnTNr-_cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks[0]"
      ],
      "metadata": {
        "id": "33mX2pyeBP2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll note that the output looks different from the first time we ran this (and had the [ starting the output). This is nicer looking, because colab can see it is a DataFrame object, which it is designed to automatically format nicely. Now we can save the result and verify that it is a DataFrame:"
      ],
      "metadata": {
        "id": "El--vPC_BbAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks_df = failed_banks[0]"
      ],
      "metadata": {
        "id": "iUntqhzbBtPf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and sure enough, in the {X} variable table we see that `failed_banks_df` is a DataFrame object.\n",
        "\n",
        "Recall *attributes* and *methods* from the tutorial. Let's run one of each on our new DataFrame object. Here is the main online reference for the [Pandas DataFrame object](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame). Scroll down a little and you will find a list of Attributes and then Methods. You will see there are many. Let's see what the names of the columns in our DataFrame are:"
      ],
      "metadata": {
        "id": "EI7LewTGB8IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAn0gU1EClEY",
        "outputId": "13e9046e-f05f-4b9d-d601-dc61084dd44a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Bank NameBank', 'CityCity', 'StateSt', 'CertCert',\n",
              "       'Acquiring InstitutionAI', 'Closing DateClosing', 'FundFund'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and let's perform a quick little analysis: count up how many failed banks there are by state. For that we use a `groupby()` method, which applies the requested analysis method (which is the `count()` method; see the document website) to the subgroups defined by `groupby()`. Which column is the state? We already saw that by using our `columns` method above. Note we can link multiple methods together by *chaining*, which is feeding the result from one method right into the next method. So our analysis looks like this:"
      ],
      "metadata": {
        "id": "_bQ3MK83DbLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks_df.groupby('StateSt').count()"
      ],
      "metadata": {
        "id": "Ebu9BVGIEKfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has done what we want - a basic count by state. There are a lot of states, and if we are interested in the states with the largest # of failed banks, sifting randomly through the entire list is inefficient. So, another method we see available to us is a sorting method, termed `sort_values()`. Let's try that:"
      ],
      "metadata": {
        "id": "kRJqi50oGeCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks_df.groupby('StateSt').count().sort_values()"
      ],
      "metadata": {
        "id": "HRirCUwrFU9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This didn't work, and always take careful note of the errors that get thrown. They will usually tell you what's going wrong. In this case, it is telling us *DataFrame.sort_values() missing 1 required positional argument: 'by'* - so we are missing an argument to the method, which tells it on which variable to perform the sort. Let's fix that; and because they all have the same count values, we can pick any one as the sort variable:"
      ],
      "metadata": {
        "id": "18w3WhdIG-A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks_df.groupby('StateSt').count().sort_values(by='CityCity')"
      ],
      "metadata": {
        "id": "1NcXd69HFRDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Better, but now we see that the *default* behavior of the method is to sort ascending. We want to see the states with the largest values at the top, which is a descending sort. Reference to the documents indicates that the argument `ascending` is a **boolean** argument, so we can specify `ascending=False` to tell the method, no we want a descending sort. Like this:"
      ],
      "metadata": {
        "id": "S4Y9rdEDHgVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks_df.groupby('StateSt').count().sort_values(by='CityCity',ascending=False)"
      ],
      "metadata": {
        "id": "fhbxcwbMFM40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking better, and essentially this is the result we want. But let's take this all the way so you can see what this stuff can do. What if we want a little table showing the 5 states with the highest # of bank failures. We already have the answer, but it is not a very nice looking table. Let's fix that, starting by isolating the top five results using the `head()` method:"
      ],
      "metadata": {
        "id": "fLnAQUEXIA5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks_df.groupby('StateSt').count().sort_values(by='CityCity',ascending=False).head()"
      ],
      "metadata": {
        "id": "rxlpe6HmEc8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Better still. Now let's turn to all of the repeated totals in each column. Really we need only one column. To do that we're going to return to indexing, only this time it is DataFrame indexing. We will pick two columns: the \"StateSt\" colulmn, which we need because that's what our counts are generated off of, and any other column - let's pick \"Bank NameBank\" (And note there is a space in the variable name!). Recall the indexing syntax above []. We use it here, and because we are specifying more than one column name, we need to provide a list of values, so there is another []. It looks like this:"
      ],
      "metadata": {
        "id": "_rByA_opIdXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks_df[['Bank NameBank','StateSt']].groupby('StateSt').count().sort_values(by='Bank NameBank',ascending=False).head()"
      ],
      "metadata": {
        "id": "Kzu7Kj0UFgjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Better still. The last piece of cleaning up to do is to give the column a proper name. \"Bank NameBank\" is no longer reflecting the data the column actually has in it, so we need to *rename* the colulmn using the `rename()` method. Again, checking the doc page for DataFrames, we see the `rename()` method has an argument we can use, `columns={}`. Recall from class that the {} indicates a *dictionary* object in python, which specifies a \"value:attribute\" or \"key:attribute\" pair. In this case, it is an \"oldname:newname\" pair, like this:"
      ],
      "metadata": {
        "id": "jJmWOtQ5JJtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_banks_df[['Bank NameBank','StateSt']].groupby('StateSt').count().sort_values(by='Bank NameBank',ascending=False).head().rename(columns={'Bank NameBank':'counts'})"
      ],
      "metadata": {
        "id": "CSGzBPQKGNSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we have our final result.  We have chained together an amazing five separate operations, beyond the original DataFrame subset. Python is really something! If you had programmed in older languages, you would be quite impressed by this, as I was.\n",
        "\n",
        "For your amusement, try to get a table showing the 10 states with the lowest number of bank failures!"
      ],
      "metadata": {
        "id": "X7oH6AHTKCwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reader: pd.read_xml()\n",
        "\n",
        "Now we look at the second markup language reader, the one for the newer <b>Extensible Markup Language</b>.\n",
        "\n",
        "I haven't been employing the directory/filename specification, but let's do it here. We will define a variable for our directory and a second variable for our filename, like this:"
      ],
      "metadata": {
        "id": "jv54C2cKKNuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"230/ascii_class/\"\n",
        "fn  = \"NF03OB0039_Monthly.xml\""
      ],
      "metadata": {
        "id": "yzJctKBSLOul"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you look at the list of variables under {X} you will see them as type STR for string. Recall the + in this context joins two strings together. Now run the `read_xml()` function:"
      ],
      "metadata": {
        "id": "Wi4SZUb-LcKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_xml(dir+fn)"
      ],
      "metadata": {
        "id": "Mpt7V7lVLY4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Works!"
      ],
      "metadata": {
        "id": "GVnELuolMD2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reader: pd.read_json()\n",
        "\n",
        "<b>Javascript Object Notation</b> is another very common format for data sharing and transfer. Let's open one up. All we have to do is chang the value for the `fn` variable to the new file name and run our function:"
      ],
      "metadata": {
        "id": "8_YziL_5MHy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn = \"combined.json\"\n",
        "pd.read_json(dir+fn)"
      ],
      "metadata": {
        "id": "Cw-sV4iUMdLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Worked. And then we can practice using some of the methods, as we did above. For example, I'm going to let you figure out how we can count how many of each species is present in the table.\n",
        "\n",
        "Have fun! Remember to try things; you can't break it."
      ],
      "metadata": {
        "id": "IdFOnJgWMm6f"
      }
    }
  ]
}